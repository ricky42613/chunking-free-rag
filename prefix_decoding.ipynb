{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question, article):\n",
    "    prompt = f'''You are a friendly chatbot who always responds in the style of a pirate. Below is an article, read the article and answer my question after the article. Now the article begins:{article} Now the article ends. Select several sentences from the article to answer my question. Question: {question}'''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_knowledge.txt', 'r') as f:\n",
    "    search_rst = f.read().replace('\\\\n', '\\n')\n",
    "    sents_in_rst = [s for s in seg.segment(search_rst)]\n",
    "question = 'What\\'s the most significant news related to cybersecurity in this week? '\n",
    "full_prompt = construct_prompt(question, search_rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "ouputs = model.model(**inputs, return_dict=True)\n",
    "logits = model.lm_head(ouputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token = {v:k for k,v in tokenizer.get_vocab().items()}\n",
    "all_prefix = set()\n",
    "prefix2sents = {}\n",
    "for id_, s in enumerate(sents_in_rst):\n",
    "    ptr = prefix2sents\n",
    "    input_ids = tokenizer(s, return_tensors=\"pt\")['input_ids'][0]\n",
    "    for i in range(1,4):\n",
    "        prefix_id = int(input_ids[i])\n",
    "        if prefix_id not in ptr:\n",
    "            ptr[prefix_id] = {'sents': []}\n",
    "        ptr[prefix_id]['sents'].append(id_)\n",
    "        ptr = ptr[prefix_id]\n",
    "        all_prefix.add(prefix_id)\n",
    "len(all_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "def predict_next_token(model, hidden_state):\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    logits = logits.float()\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    next_token_probs = torch.softmax(next_token_logits, -1)\n",
    "    return next_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_match_sents(toks):\n",
    "    ptr = prefix2sents\n",
    "    for t in toks:\n",
    "        if t not in ptr:\n",
    "            return []\n",
    "        ptr = ptr[t]\n",
    "    return ptr['sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "def get_topk_prefixes(model, current_status, top_k = 2):\n",
    "    ret = []\n",
    "    status = current_status\n",
    "    for _ in tqdm(range(5)):\n",
    "        next_status = []\n",
    "        for output, score, prev_toks in status:\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "            next_token_probs = predict_next_token(model, last_hidden_state)\n",
    "            top_k_candidates = []\n",
    "            for id in all_prefix:\n",
    "                if len(top_k_candidates) >= top_k:\n",
    "                    if top_k_candidates[0][0] < next_token_probs[0][id]:\n",
    "                        heapq.heappop(top_k_candidates)\n",
    "                        heapq.heappush(top_k_candidates, (next_token_probs[0][id], id))\n",
    "                else:\n",
    "                    heapq.heappush(top_k_candidates, (next_token_probs[0][id], id))\n",
    "            for prob, prefix_id in top_k_candidates:\n",
    "                next_input_ids = torch.tensor([prefix_id])[:, None]\n",
    "                next_attn_mask =  torch.tensor([1])[:, None]\n",
    "                new_input = {'input_ids': next_input_ids, 'attention_mask': next_attn_mask}\n",
    "                past_key_values = output.past_key_values\n",
    "                new_ouput = model.model(**new_input, past_key_values=past_key_values, return_dict=True)\n",
    "                new_prev_toks = prev_toks + [prefix_id]\n",
    "                new_score = score+math.log(prob)\n",
    "                match_prefix = prefix_match_sents(new_prev_toks)\n",
    "                if len(match_prefix) == 1:\n",
    "                    ret.append({'toks': new_prev_toks, 'score': new_score/len(new_prev_toks), 'idx': match_prefix[0]})\n",
    "                if len(match_prefix) > 0:\n",
    "                    next_status.append((new_ouput, new_score, new_prev_toks))\n",
    "            status = next_status\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "topk_prefixes = get_topk_prefixes(model, [(ouputs, 0, [])], k)\n",
    "topk_prefixes.sort(key=lambda item: item['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sent_index(promptToks, prefixToks):\n",
    "    prefix_len = len(prefixToks)\n",
    "    prefix_str = ''.join([id2token[id_].replace('▁', ' ') for id_ in prefixToks])\n",
    "    for i in range(len(promptToks)):\n",
    "        tmp = ''.join([id2token[int(id_)].replace('▁', ' ') for id_ in promptToks[i:i+prefix_len]])\n",
    "        if prefix_str.strip() == tmp.strip():\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_of_seq(beg_idx, logits, end_of_sent='</s>', distance=200):\n",
    "    len_of_prompt = len(inputs.input_ids[0])\n",
    "    end_of_sent_id = tokenizer.get_vocab()[end_of_sent]\n",
    "    max_score = float('-inf')\n",
    "    max_idx = -1\n",
    "    for i in range(beg_idx, min(beg_idx+distance, len_of_prompt)):\n",
    "        end_score = float(logits[0][i][end_of_sent_id])\n",
    "        if max_score < end_score:\n",
    "            max_score = end_score\n",
    "            max_idx = i\n",
    "    return max_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The US CISA was also affected by an Ivanti system vulnerability at the beginning of the year. Hackers targeted the US federal high-risk chemical critical infrastructure, infiltrating the Chemical Security Assessment Tool (CSAT) provided by CISA and successfully deploying a Web Shell. Another key infrastructure cybersecurity information tool, CISA Gateway, was also affected.\n",
      "- The OWASP Foundation, well-known for publishing the top ten web application security risks, recently issued a data breach notification. Member resume files from 2006 to 2014 may have been leaked due to a configuration issue on an old Wiki web server.\n"
     ]
    }
   ],
   "source": [
    "prompt_toks = [int(i) for i in inputs.input_ids[0]]\n",
    "for cand in topk_prefixes:\n",
    "    sent_prefix = tokenizer.batch_decode([cand['toks']], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    score = cand['score']\n",
    "    sent_beg_idx = find_sent_index(inputs.input_ids[0], cand['toks'])\n",
    "    end_idx = find_end_of_seq(sent_beg_idx, logits)\n",
    "    high_quality_sec = inputs.input_ids[0][sent_beg_idx: end_idx]\n",
    "    print(tokenizer.batch_decode([high_quality_sec], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunk-free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
